{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58fa3b2a",
   "metadata": {},
   "source": [
    "\n",
    "## Q1: Define the z-statistic and explain its relationship to the standard normal distribution. How is the z-statistic used in hypothesis testing?\n",
    "\n",
    "**Answer**: \n",
    "The **z-statistic**, also known as the **z-score**, is a statistical measurement that represents the number of standard deviations a data point is from the mean of a dataset. It is calculated as:\n",
    "\n",
    "\\[ z = \\frac{(X - \\mu)}{\\sigma} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the value of the sample,\n",
    "- \\( \\mu \\) is the mean of the population, and\n",
    "- \\( \\sigma \\) is the standard deviation of the population.\n",
    "\n",
    "In hypothesis testing, the z-score is used when we want to understand the relationship of a sample mean to a population mean when the population standard deviation is known.\n",
    "\n",
    "### Relationship to the Standard Normal Distribution\n",
    "The z-score transforms any normal distribution to the **standard normal distribution**, which has a mean of 0 and a standard deviation of 1. This allows for comparisons across different data sets and applications, as well as looking up probabilities and critical values using the standard normal table.\n",
    "\n",
    "### Use in Hypothesis Testing\n",
    "In hypothesis testing, the z-statistic is used as follows:\n",
    "1. **Null Hypothesis (H0)**: This is the statement that there is no effect or difference, and any observed effect is due to random chance.\n",
    "2. **Calculate the z-score** for the observed data.\n",
    "3. **Determine the p-value** associated with the z-score, which tells us the probability of obtaining a result at least as extreme as the observed data, under the null hypothesis.\n",
    "4. **Make a Decision**: If the p-value is below a predefined significance level (like 0.05), we reject the null hypothesis, suggesting that the observed effect is statistically significant.\n",
    "\n",
    "Thus, the z-score helps us determine if our observed data is far enough from the expected result under the null hypothesis to suggest that it's not due to random chance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e254126",
   "metadata": {},
   "source": [
    "\n",
    "## Q2: What is a p-value, and how is it used in hypothesis testing? What does it mean if the p-value is very small (e.g., 0.01)?\n",
    "\n",
    "**Answer**: \n",
    "The **p-value** in hypothesis testing is a measure that helps us determine the significance of our results in relation to a null hypothesis. Specifically, it quantifies the probability of observing results as extreme as (or more extreme than) the ones from our sample data, assuming that the null hypothesis is true.\n",
    "\n",
    "### Interpretation and Usage in Hypothesis Testing\n",
    "1. **Significance Level (\\(\\alpha\\))**: Before conducting a test, researchers typically set a significance level (e.g., 0.05 or 5%). This threshold indicates how much risk of error they are willing to accept.\n",
    "2. **Decision Making**: If the calculated p-value is less than or equal to \\(\\alpha\\), the results are considered statistically significant, and we reject the null hypothesis.\n",
    "3. **Understanding Small P-values**: \n",
    "   - A small p-value (e.g., 0.01) suggests that the observed data is unlikely under the null hypothesis and provides stronger evidence against it.\n",
    "   - This low probability implies that either a rare event occurred or that the null hypothesis may not be true.\n",
    "\n",
    "In summary, a very small p-value indicates strong evidence against the null hypothesis, prompting us to consider rejecting it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d01406",
   "metadata": {},
   "source": [
    "\n",
    "## Q3: Compare and contrast the binomial and Bernoulli distributions.\n",
    "\n",
    "**Answer**: \n",
    "The **Bernoulli distribution** and the **binomial distribution** are closely related and are often used in probability theory and statistics to model binary outcomes.\n",
    "\n",
    "### Bernoulli Distribution\n",
    "- The Bernoulli distribution describes a **single trial** with only two possible outcomes: success (1) with probability \\( p \\) or failure (0) with probability \\( 1 - p \\).\n",
    "- Example: A single coin flip where heads is coded as 1 (success) and tails as 0 (failure) follows a Bernoulli distribution.\n",
    "\n",
    "### Binomial Distribution\n",
    "- The binomial distribution describes the **number of successes** in a **fixed number of independent Bernoulli trials** (\\( n \\)) with a constant probability of success (\\( p \\)).\n",
    "- The probability mass function for the binomial distribution is given by:\n",
    "\\[ P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\]\n",
    "where \\( k \\) is the number of successes.\n",
    "\n",
    "### Relationship\n",
    "- The Bernoulli distribution is a **special case** of the binomial distribution where \\( n = 1 \\).\n",
    "- Both are used to model binary outcomes, but the binomial distribution considers the accumulation of successes across multiple trials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925946f0",
   "metadata": {},
   "source": [
    "\n",
    "## Q4: Under what conditions is the binomial distribution used, and how does it relate to the Bernoulli distribution?\n",
    "\n",
    "**Answer**: \n",
    "The **binomial distribution** is used when a random experiment meets the following conditions:\n",
    "\n",
    "1. **Fixed Number of Trials**: The experiment is repeated a fixed number of times (\\( n \\)).\n",
    "2. **Binary Outcomes**: Each trial has only two possible outcomes, often labeled as success and failure.\n",
    "3. **Constant Probability**: The probability of success (\\( p \\)) remains the same for each trial.\n",
    "4. **Independence**: Each trial is independent of the others.\n",
    "\n",
    "### Relationship to Bernoulli Distribution\n",
    "Each trial in a binomial distribution can be thought of as a Bernoulli trial, where the outcome is either a success or a failure. The binomial distribution is essentially the **sum of multiple independent Bernoulli trials** with the same probability of success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88256da4",
   "metadata": {},
   "source": [
    "\n",
    "## Q5: What are the key properties of the Poisson distribution, and when is it appropriate to use this distribution?\n",
    "\n",
    "**Answer**: \n",
    "The **Poisson distribution** is a discrete probability distribution that models the number of events occurring in a fixed interval of time or space, given that the events occur with a known constant rate and are independent of each other.\n",
    "\n",
    "### Key Properties\n",
    "1. **Mean and Variance**: The mean (\\( \\lambda \\)) of a Poisson distribution is equal to its variance.\n",
    "2. **Probability Mass Function**:\n",
    "   \\[ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\]\n",
    "   where \\( \\lambda \\) is the average rate of occurrence within the interval, and \\( k \\) is the number of occurrences.\n",
    "3. **Skewness**: The Poisson distribution is positively skewed, especially when \\( \\lambda \\) is small.\n",
    "\n",
    "### Appropriate Use Cases\n",
    "The Poisson distribution is typically used for **rare events** in a large sample space, such as:\n",
    "- Counting the number of customer arrivals at a store in an hour.\n",
    "- Measuring the number of typos in a book.\n",
    "- Counting the number of emails received per day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b34faa",
   "metadata": {},
   "source": [
    "\n",
    "## Q6: Define the terms \"probability distribution\" and \"probability density function\" (PDF). How does a PDF differ from a probability mass function (PMF)?\n",
    "\n",
    "**Answer**: \n",
    "- A **probability distribution** specifies how probabilities are distributed over the values of a random variable.\n",
    "- A **Probability Density Function (PDF)** describes the probability distribution for a **continuous random variable**. It provides the relative likelihood of a variable being close to a given value. The area under the PDF curve over an interval represents the probability that the variable falls within that interval.\n",
    "\n",
    "### Difference between PDF and PMF\n",
    "- **Probability Density Function (PDF)**: Used for continuous variables. For a continuous random variable \\( X \\), the probability that \\( X \\) takes on a specific value is zero, but we can calculate the probability over an interval.\n",
    "- **Probability Mass Function (PMF)**: Used for discrete random variables, where each possible value has a specific probability.\n",
    "\n",
    "For example, in a die roll (discrete variable), the PMF might show that each face has a 1/6 chance. In contrast, for a continuous variable like temperature, a PDF would represent the probability over ranges (e.g., 20-21Â°C), not individual points.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
